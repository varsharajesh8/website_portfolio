# -*- coding: utf-8 -*-
"""Rajesh_LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZqfByK-NPih_0QsRm2t0yHIHJhGPHcw1
"""

#!/usr/bin/env python3

import os
import re
import sys
import random
import torch
import torch.nn as nn
import torch.nn.functional as F

LAST_NAME = "Rajesh"

# Hyperparameters
DEFAULT_DIM = 256
DEFAULT_NUM_GEN_WORDS = 200
DEFAULT_LR = 0.0003
DEFAULT_EPOCHS = 100
FILE_PATH = "Coleridge_test.txt"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Homemade LSTM
class HomemadeLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim=DEFAULT_DIM, hidden_dim=DEFAULT_DIM):
        super(HomemadeLSTM, self).__init__()
        self.sigmoid = nn.Sigmoid()
        self.tanh = nn.Tanh()
        concat_size = embed_dim + hidden_dim

        self.forget_wts = nn.Linear(concat_size, hidden_dim)
        self.sig_inp_wts = nn.Linear(concat_size, hidden_dim)
        self.tanh_inp_wts = nn.Linear(concat_size, hidden_dim)
        self.out_wts = nn.Linear(concat_size, hidden_dim)

        self.embeddings = nn.Embedding(vocab_size, embed_dim)
        self.r2o = nn.Linear(hidden_dim, embed_dim)

        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim

    def concat(self, emb, hidden):
        if emb.dim() == 1:
            emb = emb.unsqueeze(0)
        if hidden.dim() == 1:
            hidden = hidden.unsqueeze(0)
        return torch.cat((emb, hidden), dim=1)

    def forget_gate(self, concatenation):
        return self.sigmoid(self.forget_wts(concatenation))

    def input_gate_sig(self, concatenation):
        return self.sigmoid(self.sig_inp_wts(concatenation))

    def input_gate_tanh(self, concatenation):
        return self.tanh(self.tanh_inp_wts(concatenation))

    def output_gate(self, concatenation, cell_state):
        out_gate = self.sigmoid(self.out_wts(concatenation))
        return out_gate * self.tanh(cell_state)

    def forward(self, wd_indices):
        preds = []
        hidden = None
        cell_state = None
        for ix, wd_ix in enumerate(wd_indices):
            emb = self.embeddings(wd_ix)
            if ix == 0:
                hidden = torch.zeros(1, self.hidden_dim, device=emb.device)
                cell_state = torch.zeros(1, self.hidden_dim, device=emb.device)
            concatenation = self.concat(emb, hidden)
            forget_val = self.forget_gate(concatenation)
            cell_state = cell_state * forget_val
            input_sig = self.input_gate_sig(concatenation)
            input_tanh = self.input_gate_tanh(concatenation)
            cell_state = cell_state + (input_sig * input_tanh)
            hidden = self.output_gate(concatenation, cell_state)
            preds.append(self.r2o(hidden))
        preds = torch.stack(preds, dim=0).squeeze(1)
        return preds

    def single_cell(self, wd_ix, prev_cell_state=None, prev_hidden=None):
        emb = self.embeddings(wd_ix)
        if prev_hidden is None:
            prev_hidden = torch.zeros(1, self.hidden_dim, device=emb.device)
        if prev_cell_state is None:
            prev_cell_state = torch.zeros(1, self.hidden_dim, device=emb.device)
        concatenation = self.concat(emb, prev_hidden)
        forget_val = self.forget_gate(concatenation)
        cell_state = prev_cell_state * forget_val
        input_sig = self.input_gate_sig(concatenation)
        input_tanh = self.input_gate_tanh(concatenation)
        cell_state = cell_state + (input_sig * input_tanh)
        hidden = self.output_gate(concatenation, cell_state)
        out = self.r2o(hidden)
        return out, cell_state, hidden

# Word Generation
class WordGeneration:
    def __init__(self, file_path=FILE_PATH, num_words_to_generate=DEFAULT_NUM_GEN_WORDS,
                 dim=DEFAULT_DIM, lr=DEFAULT_LR):
        self.file_path = file_path
        self.dim = dim
        self.lr = lr
        self.num_words_to_generate = num_words_to_generate
        self.lines = []
        self.words = []
        self.starting_words = []
        self.wd2ix = {}
        self.ix2wd = {}
        self._preprocess_file()
        vocab_size = len(self.words)
        if vocab_size == 0:
            raise RuntimeError(f"No words found in file: {file_path}")
        self.model = HomemadeLSTM(vocab_size, dim, dim).to(DEVICE)
        self.criterion = nn.MSELoss()
        self.optimiser = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        for i, w in enumerate(self.words):
            self.wd2ix[w] = i
            self.ix2wd[i] = w
        self.checkpoint_path = f"{LAST_NAME}_Coleridge_checkpt_dim{dim}_lr{lr}.pth"

    def _preprocess_file(self):
        if not os.path.exists(self.file_path):
            print(f"Warning: training file not found: {self.file_path}")
            return
        with open(self.file_path, "r", encoding="utf-8") as fh:
            for raw_line in fh:
                line = raw_line.rstrip("\n")
                if len(line) < 3 or line.startswith("["):
                    continue
                line = line + " BREAK"
                line = re.sub(r'([.,;:!?()“”"])', r' \1', line)
                line = re.sub(r"[_`‘]", "", line)
                line = re.sub(r"(?<!\w)'(?!\w)", "", line)
                line = re.sub(r'(--|—|–|-)', r' \1 ', line)
                line = re.sub(r'\d+', '', line)
                line = re.sub(r'\s+', ' ', line).strip()
                tokens = line.split(" ")
                for w in tokens:
                    if w not in self.words:
                        self.words.append(w)
                if len(tokens) > 0:
                    start_word = tokens[0]
                    if start_word not in self.starting_words:
                        self.starting_words.append(start_word)
                self.lines.append(tokens)

    def save_checkpoint(self):
        torch.save({'net_state_dict': self.model.state_dict(),
                    'optimiser_state_dict': self.optimiser.state_dict()},
                   self.checkpoint_path)
        print("Saving checkpoint")

    def load_checkpoint(self):
        if os.path.exists(self.checkpoint_path):
            checkpoint = torch.load(self.checkpoint_path, map_location=DEVICE)
            self.model.load_state_dict(checkpoint['net_state_dict'])
            self.optimiser.load_state_dict(checkpoint['optimiser_state_dict'])
            print("Loading checkpoint")

    def train(self, epochs=DEFAULT_EPOCHS, temperature=0.8):
        self.model.train()
        device = DEVICE
        self.load_checkpoint()
        for epoch in range(epochs):
            epoch_loss = 0.0
            prev_line = None
            for line_as_list in self.lines:
                seq = prev_line + line_as_list if prev_line else line_as_list[:]
                prev_line = line_as_list
                idx_list = [self.wd2ix[w] for w in seq if w in self.wd2ix]
                if len(idx_list) < 2:
                    continue
                input_indices = torch.tensor(idx_list[:-1], dtype=torch.long, device=device)
                target_indices = torch.tensor(idx_list[1:], dtype=torch.long, device=device)
                preds = self.model(input_indices)
                if preds.dim() == 3 and preds.size(1) == 1:
                    preds = preds.squeeze(1)
                targets = self.model.embeddings(target_indices)
                loss = self.criterion(preds, targets)
                self.optimiser.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
                self.optimiser.step()
                epoch_loss += loss.detach().item()
            print(f"epoch {epoch} loss {epoch_loss:.4f}")
            self.save_checkpoint()

            if epoch > 0 and epoch % 10 == 0:
                self.load_checkpoint()
                self.generate_text(temperature=temperature)

    def generate_text(self, temperature=0.8):
        self.model.eval()
        if not self.starting_words:
            print("No starting words available for generation.")
            return
        current_word = random.choice(self.starting_words)
        cell_state = None
        hidden = None
        generated = [current_word]
        with torch.no_grad():
            for _ in range(self.num_words_to_generate):
                wd_ix = torch.tensor([self.wd2ix[current_word]], dtype=torch.long, device=DEVICE)
                pred, cell_state, hidden = self.model.single_cell(wd_ix, cell_state, hidden)
                logits = torch.matmul(self.model.embeddings.weight, pred.squeeze(0))

                next_ix = int(torch.argmax(logits).item())
                predicted_word = self.ix2wd[next_ix]
                generated.append(predicted_word)
                current_word = predicted_word

        print("\n--- Generated text ---\n")
        line_buf = []
        for w in generated:
            if w == "BREAK":
                print(" ".join(line_buf))
                line_buf = []
            else:
                line_buf.append(w)
        if line_buf:
            print(" ".join(line_buf))

def main():
    num_epochs = DEFAULT_EPOCHS
    try:
        if len(sys.argv) > 1:
            num_epochs = int(sys.argv[1])
    except Exception:
        print(f"Invalid argument for num_epochs; using default: {DEFAULT_EPOCHS}")
        num_epochs = DEFAULT_EPOCHS

    print(f"Usage: python script.py [num_epochs]. Using default: {num_epochs}")
    print(f"Training on file: {FILE_PATH}")

    wg = WordGeneration(
        file_path=FILE_PATH,
        num_words_to_generate=DEFAULT_NUM_GEN_WORDS,
        dim=DEFAULT_DIM,
        lr=DEFAULT_LR
    )
    wg.train(epochs=num_epochs, temperature=0.8)
    wg.generate_text(temperature=0.8)

if __name__ == "__main__":
    main()
    print("\na. The generated text has the general style and vocab of the original file but does not reproduce exactly.")
    print("b. There are discontinuities. Some lines change context very quickly and start talking about different things. There are also repetitions.")
    print("c. It was around 5 tens of epochs where we see non-repetitive lines.")